{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.lebledparle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### actualites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL='https://www.lebledparle.com/fr/'\n",
    "htmlText = requests.get(URL).text\n",
    "#print(htmlText)\n",
    "data=[]\n",
    "soup=BeautifulSoup(htmlText, 'html.parser')\n",
    "elt=soup.find('div',class_='t3-main-content col col-xs-12 col-sm-8')\n",
    "elt2 = elt.find_all('h4',class_=\"mod-articles-category-title\")\n",
    "for item in elt2:\n",
    "    contenu=\"https://www.lebledparle.com\"+str(item.find('a').get('href'))\n",
    "    source='le bled parle'\n",
    "    title=item.find('a').text\n",
    "    label=0\n",
    "    theme='actualite'\n",
    "    #print(f'auteur :{auteur}, title:{title}, contenu:{contenu} \\n')\n",
    "    data.append({\n",
    "    'source':source,\n",
    "    'author':'',\n",
    "    'theme':theme,\n",
    "    'title':title,\n",
    "    'contenu':contenu,\n",
    "    'label':0\n",
    "    })\n",
    "#print(data)\n",
    "for ind2,cont in enumerate(data):\n",
    "    #print(f'auteur :{cont.auteur}, title:{cont.title}, contenu:{cont.contenu} \\n')\n",
    "    #print(cont['contenu'])\n",
    "    req=requests.get(cont['contenu']).text\n",
    "    content=BeautifulSoup(req, 'html.parser')\n",
    "    content1=content.find('section',class_=\"article-content clearfix\")\n",
    "    cont1=content1.find_all('p',class_=\"\")\n",
    "    #print(cont1)\n",
    "    data[ind2]['contenu']=\"\"\n",
    "    for ite in cont1:\n",
    "        data[ind2]['contenu']=data[ind2]['contenu']+\" \"+str(ite.text)\n",
    "#print(data[12]['contenu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##create pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCSV(data):   \n",
    "    data1=[]\n",
    "    \"\"\"\n",
    "    'auteur':auteur,\n",
    "        'title':title,\n",
    "        'contenu':contenu,\n",
    "        'label':0\n",
    "    \"\"\"\n",
    "    i=0\n",
    "    for dat in data:\n",
    "        #print(dat['auteur'])\n",
    "        data1.append([i, str(dat['source']), str(dat['title']), str(dat['contenu']),str(dat['author']),str(dat['theme']), str(dat['label'])])\n",
    "        i=i+1\n",
    "    #print(data1)\n",
    "    columns=['id','source','title','text','author','theme','label']\n",
    "    bledDataNews=pd.DataFrame(data=data1,columns=columns)\n",
    "    bledDataNews.head()\n",
    "    return bledDataNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "nameFile1=\"bledDataNews_actualite\"+str(today)+\".csv\"\n",
    "getCSV(data).to_csv(nameFile1, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### faits divers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.lebledparle.com/fr/bled\n",
    "URL='https://www.lebledparle.com/fr/bled'\n",
    "htmlText = requests.get(URL).text\n",
    "#print(htmlText)\n",
    "data=[]\n",
    "soup=BeautifulSoup(htmlText, 'html.parser')\n",
    "elt=soup.find('div',class_='t3-content col col-xs-12 col-sm-8')\n",
    "elt2 = elt.find_all('div',class_=\"items-row\")\n",
    "#print(elt2)\n",
    "for elt in elt2:\n",
    "    for item in elt.find_all('div',class_='col-sm-6'):\n",
    "        contenu=URL+str(item.find('div',class_=\"pull- item-image\").find('a').get('href'))\n",
    "        source='le bled parle'\n",
    "        title=item.find('h3',class_=\"article-title\").text\n",
    "        label=0\n",
    "        #print(f'auteur :{auteur}, title:{title} \\n')\n",
    "        data.append({\n",
    "        'source':source,\n",
    "        'author':'',\n",
    "        'theme':'faits divers',\n",
    "        'title':title,\n",
    "        'contenu':contenu,\n",
    "        'label':0\n",
    "        })\n",
    "#print(data)\n",
    "\n",
    "    \n",
    "for ind2,cont in enumerate(data):\n",
    "    #print(f'auteur :{cont.auteur}, title:{cont.title}, contenu:{cont.contenu} \\n')\n",
    "    #print(cont['contenu'])\n",
    "    req=requests.get(cont['contenu']).text\n",
    "    content=BeautifulSoup(req, 'html.parser')\n",
    "    content1=content.find('section',class_=\"article-content clearfix\")\n",
    "    cont1=content1.find_all('p',class_=\"\")\n",
    "    #print(cont1)\n",
    "    data[ind2]['contenu']=\"\"\n",
    "    for ite in cont1:\n",
    "        data[ind2]['contenu']=data[ind2]['contenu']+\" \"+str(ite.text)\n",
    "#print(data[12]['contenu'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "nameFile1=\"bledDataNews_faitsdivers\"+str(today)+\".csv\"\n",
    "getCSV(data).to_csv(nameFile1, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.lebledparle.com/fr/people\n",
    "#https://www.lebledparle.com/fr/bled\n",
    "URL='https://www.lebledparle.com/fr/people'\n",
    "htmlText = requests.get(URL).text\n",
    "#print(htmlText)\n",
    "data=[]\n",
    "soup=BeautifulSoup(htmlText, 'html.parser')\n",
    "elt=soup.find('div',class_='t3-content col col-xs-12 col-sm-8')\n",
    "elt2 = elt.find_all('div',class_=\"items-row\")\n",
    "#print(elt2)\n",
    "for elt in elt2:\n",
    "    for item in elt.find_all('div',class_='col-sm-6'):\n",
    "        contenu=URL+str(item.find('div',class_=\"pull- item-image\").find('a').get('href'))\n",
    "        source='le bled parle'\n",
    "        title=item.find('h3',class_=\"article-title\").text\n",
    "        label=0\n",
    "        #print(f'auteur :{auteur}, title:{title} \\n')\n",
    "        data.append({\n",
    "        'source':source,\n",
    "        'theme':'people',\n",
    "        'author':'',\n",
    "        'title':title,\n",
    "        'contenu':contenu,\n",
    "        'label':0\n",
    "        })\n",
    "#print(data)\n",
    "\n",
    "    \n",
    "for ind2,cont in enumerate(data):\n",
    "    #print(f'auteur :{cont.auteur}, title:{cont.title}, contenu:{cont.contenu} \\n')\n",
    "    #print(cont['contenu'])\n",
    "    req=requests.get(cont['contenu']).text\n",
    "    content=BeautifulSoup(req, 'html.parser')\n",
    "    content1=content.find('section',class_=\"article-content clearfix\")\n",
    "    cont1=content1.find_all('p',class_=\"\")\n",
    "    #print(cont1)\n",
    "    data[ind2]['contenu']=\"\"\n",
    "    for ite in cont1:\n",
    "        data[ind2]['contenu']=data[ind2]['contenu']+\" \"+str(ite.text)\n",
    "#print(data[12]['contenu'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "nameFile1=\"bledDataNews_people\"+str(today)+\".csv\"\n",
    "getCSV(data).to_csv(nameFile1, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.lebledparle.com/fr/medias\n",
    "URL='https://www.lebledparle.com/fr/medias'\n",
    "htmlText = requests.get(URL).text\n",
    "#print(htmlText)\n",
    "data=[]\n",
    "soup=BeautifulSoup(htmlText, 'html.parser')\n",
    "elt=soup.find('div',class_='t3-content col col-xs-12 col-sm-8')\n",
    "elt2 = elt.find_all('div',class_=\"items-row\")\n",
    "#print(elt2)\n",
    "for elt in elt2:\n",
    "    for item in elt.find_all('div',class_='col-sm-6'):\n",
    "        contenu=URL+str(item.find('div',class_=\"pull- item-image\").find('a').get('href'))\n",
    "        theme='media'\n",
    "        title=item.find('h3',class_=\"article-title\").text\n",
    "        label=0\n",
    "        #print(f'auteur :{auteur}, title:{title} \\n')\n",
    "        data.append({\n",
    "        'theme':theme,\n",
    "        'author':'',\n",
    "        'source':'le bled parle',\n",
    "        'title':title,\n",
    "        'contenu':contenu,\n",
    "        'label':0\n",
    "        })\n",
    "#print(data)\n",
    "    \n",
    "for ind2,cont in enumerate(data):\n",
    "    #print(f'auteur :{cont.auteur}, title:{cont.title}, contenu:{cont.contenu} \\n')\n",
    "    #print(cont['contenu'])\n",
    "    req=requests.get(cont['contenu']).text\n",
    "    content=BeautifulSoup(req, 'html.parser')\n",
    "    content1=content.find('section',class_=\"article-content clearfix\")\n",
    "    cont1=content1.find_all('p',class_=\"\")\n",
    "    #print(cont1)\n",
    "    data[ind2]['contenu']=\"\"\n",
    "    for ite in cont1:\n",
    "        data[ind2]['contenu']=data[ind2]['contenu']+\" \"+str(ite.text)\n",
    "#print(data[12]['contenu'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "nameFile1=\"bledDataNews_media\"+str(today)+\".csv\"\n",
    "getCSV(data).to_csv(nameFile1, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.lebledparle.com/fr/medias\n",
    "URL='https://www.lebledparle.com/fr/sport'\n",
    "htmlText = requests.get(URL).text\n",
    "#print(htmlText)\n",
    "data=[]\n",
    "soup=BeautifulSoup(htmlText, 'html.parser')\n",
    "elt=soup.find('div',class_='t3-content col col-xs-12 col-sm-8')\n",
    "elt2 = elt.find_all('div',class_=\"items-row\")\n",
    "#print(elt2)\n",
    "for elt in elt2:\n",
    "    for item in elt.find_all('div',class_='col-sm-6'):\n",
    "        contenu=URL+str(item.find('div',class_=\"pull- item-image\").find('a').get('href'))\n",
    "        theme='sport'\n",
    "        title=item.find('h3',class_=\"article-title\").text\n",
    "        label=0\n",
    "        #print(f'auteur :{auteur}, title:{title} \\n')\n",
    "        data.append({\n",
    "        'theme':theme,\n",
    "        'source':'le bled parle',\n",
    "        'author':'',\n",
    "        'title':title,\n",
    "        'contenu':contenu,\n",
    "        'label':0\n",
    "        })\n",
    "#print(data)\n",
    "    \n",
    "for ind2,cont in enumerate(data):\n",
    "    #print(f'auteur :{cont.auteur}, title:{cont.title}, contenu:{cont.contenu} \\n')\n",
    "    #print(cont['contenu'])\n",
    "    req=requests.get(cont['contenu']).text\n",
    "    content=BeautifulSoup(req, 'html.parser')\n",
    "    content1=content.find('section',class_=\"article-content clearfix\")\n",
    "    cont1=content1.find_all('p',class_=\"\")\n",
    "    #print(cont1)\n",
    "    data[ind2]['contenu']=\"\"\n",
    "    for ite in cont1:\n",
    "        data[ind2]['contenu']=data[ind2]['contenu']+\" \"+str(ite.text)\n",
    "#print(data[12]['contenu'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "nameFile1=\"bledDataNews_sport\"+str(today)+\".csv\"\n",
    "getCSV(data).to_csv(nameFile1, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### societe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.lebledparle.com/fr/medias\n",
    "URL='https://www.lebledparle.com/fr/societe'\n",
    "htmlText = requests.get(URL).text\n",
    "#print(htmlText)\n",
    "data=[]\n",
    "soup=BeautifulSoup(htmlText, 'html.parser')\n",
    "elt=soup.find('div',class_='t3-content col col-xs-12 col-sm-8')\n",
    "elt2 = elt.find_all('div',class_=\"items-row\")\n",
    "#print(elt2)\n",
    "for elt in elt2:\n",
    "    for item in elt.find_all('div',class_='col-sm-6'):\n",
    "        contenu=URL+str(item.find('div',class_=\"pull- item-image\").find('a').get('href'))\n",
    "        source='le bled parle'\n",
    "        title=item.find('h3',class_=\"article-title\").text\n",
    "        label=0\n",
    "        #print(f'auteur :{auteur}, title:{title} \\n')\n",
    "        data.append({\n",
    "        'source':source,\n",
    "        'author':'',\n",
    "        'theme':'societe',\n",
    "        'title':title,\n",
    "        'contenu':contenu,\n",
    "        'label':0\n",
    "        })\n",
    "#print(data)\n",
    "    \n",
    "for ind2,cont in enumerate(data):\n",
    "    #print(f'auteur :{cont.auteur}, title:{cont.title}, contenu:{cont.contenu} \\n')\n",
    "    #print(cont['contenu'])\n",
    "    req=requests.get(cont['contenu']).text\n",
    "    content=BeautifulSoup(req, 'html.parser')\n",
    "    content1=content.find('section',class_=\"article-content clearfix\")\n",
    "    cont1=content1.find_all('p',class_=\"\")\n",
    "    #print(cont1)\n",
    "    data[ind2]['contenu']=\"\"\n",
    "    for ite in cont1:\n",
    "        data[ind2]['contenu']=data[ind2]['contenu']+\" \"+str(ite.text)\n",
    "#print(data[12]['contenu'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "nameFile1=\"bledDataNews_societe\"+str(today)+\".csv\"\n",
    "getCSV(data).to_csv(nameFile1, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### politique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.lebledparle.com/fr/medias\n",
    "URL='https://www.lebledparle.com/fr/politique-cameroun'\n",
    "htmlText = requests.get(URL).text\n",
    "#print(htmlText)\n",
    "data=[]\n",
    "soup=BeautifulSoup(htmlText, 'html.parser')\n",
    "elt=soup.find('div',class_='t3-content col col-xs-12 col-sm-8')\n",
    "elt2 = elt.find_all('div',class_=\"items-row\")\n",
    "#print(elt2)\n",
    "for elt in elt2:\n",
    "    for item in elt.find_all('div',class_='col-sm-6'):\n",
    "        contenu=URL+str(item.find('div',class_=\"pull- item-image\").find('a').get('href'))\n",
    "        source='le bled parle'\n",
    "        title=item.find('h3',class_=\"article-title\").text\n",
    "        label=0\n",
    "        #print(f'auteur :{auteur}, title:{title} \\n')\n",
    "        data.append({\n",
    "        'source':source,\n",
    "        'theme':'politique',\n",
    "        'author':'',\n",
    "        'title':title,\n",
    "        'contenu':contenu,\n",
    "        'label':0\n",
    "        })\n",
    "#print(data)\n",
    "    \n",
    "for ind2,cont in enumerate(data):\n",
    "    #print(f'auteur :{cont.auteur}, title:{cont.title}, contenu:{cont.contenu} \\n')\n",
    "    #print(cont['contenu'])\n",
    "    req=requests.get(cont['contenu']).text\n",
    "    content=BeautifulSoup(req, 'html.parser')\n",
    "    content1=content.find('section',class_=\"article-content clearfix\")\n",
    "    cont1=content1.find_all('p',class_=\"\")\n",
    "    #print(cont1)\n",
    "    data[ind2]['contenu']=\"\"\n",
    "    for ite in cont1:\n",
    "        data[ind2]['contenu']=data[ind2]['contenu']+\" \"+str(ite.text)\n",
    "#print(data[12]['contenu'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "nameFile1=\"bledDataNews_politique\"+str(today)+\".csv\"\n",
    "getCSV(data).to_csv(nameFile1, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EcoMatin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContent(url):\n",
    "    htmlText1 = requests.get(url).text\n",
    "    otherSubTitle=\"\"\n",
    "    subTitle=\"\"\n",
    "    soup1=BeautifulSoup(htmlText1, 'html.parser')\n",
    "    #print(soup1)\n",
    "    if not soup1.find('h2',class_=\"entry-sub-title\") is None:\n",
    "        subTitle=soup1.find('h2',class_=\"entry-sub-title\").text\n",
    "        #find('div',class_='site-content container')\n",
    "    #print(\"kllklksd;lk;sdlk\"+str(subTitle)+\"\\n\")\n",
    "    elt=soup1.find('div',class_='entry-content entry clearfix')\n",
    "    #print(elt)\n",
    "    temp=elt.find_all('p')\n",
    "    #print(temp)\n",
    "    for e in temp:\n",
    "        otherSubTitle=otherSubTitle+\" \"+str(e.text)\n",
    "    return subTitle+\" \"+otherSubTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "URLs=['https://ecomatin.net/category/echos-des-regions/est/',\n",
    "     'https://ecomatin.net/category/echos-des-regions/ad/',\n",
    "     'https://ecomatin.net/category/echos-des-regions/centre/',\n",
    "     'https://ecomatin.net/category/echos-des-regions/littoral/',\n",
    "     'https://ecomatin.net/category/echos-des-regions/n/',\n",
    "     'https://ecomatin.net/category/echos-des-regions/ouest/',\n",
    "     'https://ecomatin.net/category/echos-des-regions/sud/',\n",
    "     'https://ecomatin.net/category/conjoncture/mines-et-energies/'\n",
    "]\n",
    "     \n",
    "\n",
    "data=[]\n",
    "for URL in URLs:\n",
    "    i=1\n",
    "    while requests.get(URL+\"page/\"+str(i)+'/').status_code == 200:\n",
    "        htmlText = requests.get(URL+\"page/\"+str(i)+'/').text\n",
    "        #print(requests.get(URL+str(i)+'/').status_code)\n",
    "        \n",
    "        soup=BeautifulSoup(htmlText, 'html.parser')\n",
    "        elt0=soup.find('div',class_=\"mag-box-container clearfix\")\n",
    "        #print(elt0)\n",
    "        elt1=elt0.find_all('li',class_=\"post-item\")\n",
    "        for item in elt1:\n",
    "            elt2=item.find('div',class_='post-details')\n",
    "            elt3 = elt2.find('h2',class_=\"post-title\").find('a')\n",
    "            data.append({\n",
    "            'source':'ecomatin',\n",
    "            'theme':'eco des regions',\n",
    "            'author':'',\n",
    "            'title':elt3.text,\n",
    "            'contenu':getContent((elt3.get('href'))),\n",
    "            'label':0\n",
    "            })    \n",
    "        i=i+1\n",
    "#print(elt)\n",
    "print(data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "nameFile1=\"ecomatin_actualite\"+str(today)+\".csv\"\n",
    "getCSV(data).to_csv(nameFile1, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Cameroun Tribune Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCSV(data):   \n",
    "    data1=[]\n",
    "    \"\"\"\n",
    "    'auteur':auteur,\n",
    "        'title':title,\n",
    "        'contenu':contenu,\n",
    "        'label':0\n",
    "    \"\"\"\n",
    "    i=0\n",
    "    for dat in data:\n",
    "        #print(dat['auteur'])\n",
    "        data1.append([i, str(dat['source']), str(dat['title']), str(dat['contenu']),str(dat['author']),str(dat['theme']), str(dat['label'])])\n",
    "        i=i+1\n",
    "    #print(data1)\n",
    "    columns=['id','source','title','text','author','theme','label']\n",
    "    bledDataNews=pd.DataFrame(data=data1,columns=columns)\n",
    "    bledDataNews.head()\n",
    "    return bledDataNews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### can cameroon/presidential/societe/culture/sports/regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "URLs=[\n",
    "    \"https://www.cameroon-tribune.cm/category2.html/13/@/fr.html/liste-des-articles\",#can cameroon\n",
    "     \"https://www.cameroon-tribune.cm/category2.html/4/@/fr.html/liste-des-articles\",#presidential\n",
    "    # \"https://www.cameroon-tribune.cm/category2.html/1/@/fr.html/liste-des-articles\",#politique\n",
    "    # \"https://www.cameroon-tribune.cm/category2.html/2/@/fr.html/liste-des-articles\",#economie\n",
    "      \"https://www.cameroon-tribune.cm/category2.html/5/@/fr.html/liste-des-articles\",#societe\n",
    "      \"https://www.cameroon-tribune.cm/category2.html/6/@/fr.html/liste-des-articles\",#culture\n",
    "      \"https://www.cameroon-tribune.cm/category2.html/8/@/fr.html/liste-des-articles\",#sports\n",
    "      \"https://www.cameroon-tribune.cm/category2.html/7/@/fr.html/liste-des-articles\"#regions\n",
    "    \n",
    "]\n",
    "\"\"\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.get(URL)\n",
    "\n",
    "time.sleep(5)\n",
    "content = driver.page_source.encode('utf-8').strip()\n",
    "\"\"\"\n",
    "\n",
    "data=[]\n",
    "headers = requests.utils.default_headers()\n",
    "\n",
    "headers.update(\n",
    "    {\n",
    "        'User-Agent': 'My User Agent 1.0',\n",
    "    }\n",
    ")\n",
    "try:\n",
    "    for URL in URLs:\n",
    "        i=1\n",
    "        #time.sleep(200)\n",
    "        while requests.get(URL.replace('@',''+str(i)), headers=headers).status_code == 200 and len(BeautifulSoup(requests.get(URL.replace('@',''+str(i)), headers=headers).text, 'html.parser').find_all('div',class_='news-list-item articles-list')) !=0:\n",
    "            htmlText1 = requests.get(URL.replace('@',''+str(i))).text\n",
    "            print(str(URL)+\"\\n\")\n",
    "            print(\"htmlText \"+str(i)+\" \"+str(requests.get(URL.replace('@',''+str(i))).status_code)+\"\\n\")\n",
    "            \n",
    "            soup1=BeautifulSoup(htmlText1, 'html.parser')\n",
    "            #print(soup1)\n",
    "            #soup2=soup1.find('div',class_=\"col-sm-8 col-p  main-content\")\n",
    "            #print(\"soup2\"+str(soup2)+\"\\n\")\n",
    "            #if not soup2.find_all('div',class_='news-list-item articles-list') is None:\n",
    "            #soup3=soup2.find_all('div',class_='news-list-item articles-list')#news-list-item articles-list\n",
    "            #print(\"soup1.find_all('div',class_='news-list-item articles-list')\"+str(soup1.find_all('div',class_='news-list-item articles-list'))+\"\\n\")\n",
    "            if not soup1.find_all('div',class_='news-list-item articles-list') is None:\n",
    "                for val in soup1.find_all('div',class_='news-list-item articles-list'):\n",
    "                    if not val.find('div',class_='post-info-2') is None:\n",
    "                        art=val.find('div',class_='post-info-2')\n",
    "                        contenu=\"https://www.cameroon-tribune.cm\"+str(art.find('h4').find('a').get('href'))\n",
    "                        auteur=str(art.find('ul',class_=\"authar-info\").find('li',class_='authar hidden-xs hidden-sm').find('a').text).replace('Par','')\n",
    "                        auteur=auteur.replace('Brussel Airline','cameroon-tribune')\n",
    "                        auteur=auteur.replace('Paul BIYA','cameroon-tribune')# Issa Tchiroma Bakary\n",
    "                        auteur=auteur.replace('Issa Tchiroma Bakary','cameroon-tribune')\n",
    "                        if URL == \"https://www.cameroon-tribune.cm/category2.html/13/@/fr.html/liste-des-articles\":\n",
    "                            theme='sport'\n",
    "                        elif URL == \"https://www.cameroon-tribune.cm/category2.html/4/@/fr.html/liste-des-articles\":\n",
    "                            theme='politique'\n",
    "                        elif URL == \"https://www.cameroon-tribune.cm/category2.html/5/@/fr.html/liste-des-articles\":\n",
    "                            theme='societe'\n",
    "                        elif URL == \"https://www.cameroon-tribune.cm/category2.html/6/@/fr.html/liste-des-articles\":\n",
    "                            theme='culture'\n",
    "                        elif URL == \"https://www.cameroon-tribune.cm/category2.html/8/@/fr.html/liste-des-articles\":\n",
    "                            theme='sport'\n",
    "                        elif URL == \"https://www.cameroon-tribune.cm/category2.html/7/@/fr.html/liste-des-articles\":\n",
    "                            theme='eco des regions'\n",
    "                        title=art.find('h4').find('a').text\n",
    "                        label=0\n",
    "                        print(f'auteur :{auteur}, title:{title}, contenu:{contenu} \\n')\n",
    "                        data.append({\n",
    "                        'author':auteur,\n",
    "                        'theme':theme,\n",
    "                        'source':'cameroun tribune',\n",
    "                        'title':title,\n",
    "                        'contenu':contenu,\n",
    "                        'label':0\n",
    "                        })\n",
    "            i=i+1\n",
    "except:\n",
    "    print(data)\n",
    "    print('\\n')\n",
    "    today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "    nameFile1=\"'Error_cameroon_tribune_with_link'\"+str(today)+\".csv\"\n",
    "    getCSV(data).to_csv(nameFile1, index=False)\n",
    "try:\n",
    "    for ind2,cont in enumerate(data):\n",
    "        #print(f'auteur :{'str(cont['auteur'])+'}, title:{cont['title']}, contenu:{cont['contenu']}+'\\n')\n",
    "        #time.sleep(1000)\n",
    "        print(\"contenu\"+cont['contenu'])\n",
    "        req=requests.get(cont['contenu'], headers=headers).text\n",
    "        content=BeautifulSoup(req, 'html.parser')\n",
    "        content1=content.find('div',class_=\"post_details_inner\")\n",
    "        cont1=content1.find('p').text\n",
    "        cont2=content1.find('div',class_=\"testID\").find('p').text\n",
    "        #print(cont1)\n",
    "        data[ind2]['contenu']=\"\"\n",
    "        data[ind2]['contenu']=str(cont2)+\" \"+str(cont2)\n",
    "except :\n",
    "    import datetime\n",
    "    today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "    nameFile1=\"Error_cameroon_tribune\"+str(today)+\".csv\"\n",
    "    getCSV(data).to_csv(nameFile1, index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "nameFile1=\"cameroon_tribune\"+str(today)+\".csv\"\n",
    "getCSV(data).to_csv(nameFile1, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### politique et economie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "URLs=[\n",
    "    #\"https://www.cameroon-tribune.cm/category2.html/13/@/fr.html/liste-des-articles\",#can cameroon\n",
    "    # \"https://www.cameroon-tribune.cm/category2.html/4/@/fr.html/liste-des-articles\",#presidential\n",
    "     \"https://www.cameroon-tribune.cm/category2.html/1/@/fr.html/liste-des-articles\",#politique\n",
    "     \"https://www.cameroon-tribune.cm/category2.html/2/@/fr.html/liste-des-articles\",#economie\n",
    "    #  \"https://www.cameroon-tribune.cm/category2.html/5/@/fr.html/liste-des-articles\",#societe\n",
    "    #  \"https://www.cameroon-tribune.cm/category2.html/6/@/fr.html/liste-des-articles\",#culture\n",
    "    #  \"https://www.cameroon-tribune.cm/category2.html/8/@/fr.html/liste-des-articles\",#sports\n",
    "    #  \"https://www.cameroon-tribune.cm/category2.html/7/@/fr.html/liste-des-articles\"#regions\n",
    "    \n",
    "]\n",
    "\"\"\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "driver.get(URL)\n",
    "\n",
    "time.sleep(5)\n",
    "content = driver.page_source.encode('utf-8').strip()\n",
    "\"\"\"\n",
    "\n",
    "data=[]\n",
    "headers = requests.utils.default_headers()\n",
    "\n",
    "headers.update(\n",
    "    {\n",
    "        'User-Agent': 'My User Agent 1.0',\n",
    "    }\n",
    ")\n",
    "try:\n",
    "    for URL in URLs:\n",
    "        i=1\n",
    "        #time.sleep(200)\n",
    "        while requests.get(URL.replace('@',''+str(i)), headers=headers).status_code == 200 and len(BeautifulSoup(requests.get(URL.replace('@',''+str(i)), headers=headers).text, 'html.parser').find_all('div',class_='news-list-item articles-list')) !=0:\n",
    "            htmlText1 = requests.get(URL.replace('@',''+str(i))).text\n",
    "            print(str(URL)+\"\\n\")\n",
    "            print(\"htmlText \"+str(i)+\" \"+str(requests.get(URL.replace('@',''+str(i))).status_code)+\"\\n\")\n",
    "            \n",
    "            soup1=BeautifulSoup(htmlText1, 'html.parser')\n",
    "            #print(soup1)\n",
    "            #soup2=soup1.find('div',class_=\"col-sm-8 col-p  main-content\")\n",
    "            #print(\"soup2\"+str(soup2)+\"\\n\")\n",
    "            #if not soup2.find_all('div',class_='news-list-item articles-list') is None:\n",
    "            #soup3=soup2.find_all('div',class_='news-list-item articles-list')#news-list-item articles-list\n",
    "            #print(\"soup1.find_all('div',class_='news-list-item articles-list')\"+str(soup1.find_all('div',class_='news-list-item articles-list'))+\"\\n\")\n",
    "            if not soup1.find_all('div',class_='news-list-item articles-list') is None:\n",
    "                for val in soup1.find_all('div',class_='news-list-item articles-list'):\n",
    "                    if not val.find('div',class_='post-info-2') is None:\n",
    "                        art=val.find('div',class_='post-info-2')\n",
    "                        contenu=\"https://www.cameroon-tribune.cm\"+str(art.find('h4').find('a').get('href'))\n",
    "                        auteur=str(art.find('ul',class_=\"authar-info\").find('li',class_='authar hidden-xs hidden-sm').find('a').text).replace('Par','')\n",
    "                        auteur=auteur.replace('Brussel Airline','cameroon-tribune')\n",
    "                        auteur=auteur.replace('Paul BIYA','cameroon-tribune')# Issa Tchiroma Bakary\n",
    "                        auteur=auteur.replace('Issa Tchiroma Bakary','cameroon-tribune')\n",
    "                        title=art.find('h4').find('a').text\n",
    "                        label=0\n",
    "                        print(f'auteur :{auteur}, title:{title}, contenu:{contenu} \\n')\n",
    "                        data.append({\n",
    "                        'auteur':auteur,\n",
    "                        'title':title,\n",
    "                        'contenu':contenu,\n",
    "                        'label':0\n",
    "                        })\n",
    "            i=i+1\n",
    "except:\n",
    "    print(data)\n",
    "    print('\\n')\n",
    "    today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "    nameFile1=\"'Error_cameroon_tribune_politique_economie'\"+str(today)+\".csv\"\n",
    "    getCSV(data).to_csv(nameFile1, index=False)\n",
    "try:\n",
    "    for ind2,cont in enumerate(data):\n",
    "        #print(f'auteur :{'str(cont['auteur'])+'}, title:{cont['title']}, contenu:{cont['contenu']}+'\\n')\n",
    "        #time.sleep(1000)\n",
    "        print(\"contenu\"+cont['contenu'])\n",
    "        req=requests.get(cont['contenu'], headers=headers).text\n",
    "        content=BeautifulSoup(req, 'html.parser')\n",
    "        content1=content.find('div',class_=\"post_details_inner\")\n",
    "        cont1=content1.find('p').text\n",
    "        cont2=content1.find('div',class_=\"testID\").find('p').text\n",
    "        #print(cont1)\n",
    "        data[ind2]['contenu']=\"\"\n",
    "        data[ind2]['contenu']=str(cont2)+\" \"+str(cont2)\n",
    "except :\n",
    "    import datetime\n",
    "    today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "    nameFile1=\"Error_cameroon_tribune_politique_economie\"+str(today)+\".csv\"\n",
    "    getCSV(data).to_csv(nameFile1, index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import datetime\n",
    "today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "nameFile1=\"cameroon_tribune_politique_economie\"+str(today)+\".csv\"\n",
    "getCSV(data).to_csv(nameFile1, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### webscraping contenu politique/economie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import validators\n",
    "\n",
    "headers = requests.utils.default_headers()\n",
    "\n",
    "headers.update(\n",
    "    {\n",
    "        'User-Agent': 'Google Chrome 5.0'#My User Agent 3.0',\n",
    "    }\n",
    ")\n",
    "\n",
    "data=pd.read_csv('Correct1_cameroon_tribune.csv')\n",
    "#print(data.loc[1,'text'])\n",
    "try:\n",
    "    for ind2,cont in enumerate(data['text']):\n",
    "        if validators.url(str(cont)):\n",
    "                #print(f'auteur :{'str(cont['auteur'])+'}, title:{cont['title']}, contenu:{cont['contenu']}+'\\n')\n",
    "                #time.sleep(1000)\n",
    "                #print(f\"contenu\"+cont+str(ind2)+'\\n')\n",
    "                \n",
    "                req=requests.get(cont, headers=headers).text\n",
    "                print(requests.get(cont, headers=headers).status_code)\n",
    "                content=BeautifulSoup(req, 'html.parser')\n",
    "                content1=content.find('div',class_=\"post_details_inner\")\n",
    "                if content1.find('p')!= None:\n",
    "                    cont1=content1.find('p').text\n",
    "                    if content1.find('div',class_=\"testID\") !=None:\n",
    "                        cont2=content1.find('div',class_=\"testID\").find('p').text\n",
    "                        print(str(cont2)+\" \"+str(cont2)+'\\n')\n",
    "                        data.loc[ind2, 'text']=\"\"\n",
    "                        data.loc[ind2, 'text']=str(cont2)+\" \"+str(cont2)\n",
    "except :\n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "    nameFile1=\"error__cameroon_tribune2022-03-20\"+str(today)+\".csv\"\n",
    "    data.to_csv(nameFile1, index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "today=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "nameFile1=\"Correct2__cameroon_tribune\"+str(today)+\".csv\"\n",
    "data.to_csv(nameFile1, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Url is valid\n"
     ]
    }
   ],
   "source": [
    "import validators\n",
    "valid=validators.url('https://www.cameroon-tribune.cm/article.html/44741/fr.html/cameroun-comores-')\n",
    "print(valid)\n",
    "if validators.url('https://www.cameroon-tribune.cm/article.html/8779/fr.html/audience-au-palais-de-lunite-le-president-de-la-republique-recu-mercredi-mme-cristina-rivero-de-theisen-consultante-internationale'):\n",
    "    print(\"Url is valid\")\n",
    "else:\n",
    "    print(\"Invalid url\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34facd98c0ce5419163ca97320a9b69bd75135fd295ec44b93c4fe55ca14c732"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
